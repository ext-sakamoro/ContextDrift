#!/usr/bin/env python3
"""
Context Drift Detection Test Runner
Executes Context Drift tests against various LLM APIs (Claude, Gemini, OpenAI)

Usage:
    python run_context_drift_api.py --api claude --model claude-sonnet-4-5
    python run_context_drift_api.py --api gemini --model gemini-2.0-flash-exp
    python run_context_drift_api.py --api openai --model gpt-4o
"""

import os
import sys
import json
import argparse
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
import re

# API clients
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    print("Warning: anthropic package not installed. Install with: pip install anthropic")

try:
    import google.generativeai as genai
    GOOGLE_AVAILABLE = True
except ImportError:
    GOOGLE_AVAILABLE = False
    print("Warning: google-generativeai package not installed. Install with: pip install google-generativeai")

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("Warning: openai package not installed. Install with: pip install openai")

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    print("Warning: numpy not installed. Install with: pip install numpy")

import random

# Othello board size / ã‚ªã‚»ãƒ­ç›¤ã‚µã‚¤ã‚º
SIZE = 8


# --- Game Physics System for Context Drift ---
class GamePhysics:
    """
    ã‚²ãƒ¼ãƒ ç‰©ç†ã‚·ã‚¹ãƒ†ãƒ  / Game Physics System
    - ãƒˆãƒãƒ­ã‚¸ãƒ¼å¤‰æ›´ã®ç®¡ç† / Manages topology changes
    - å¹»å½±çŸ³ï¼ˆãƒ•ã‚¡ãƒ³ãƒˆãƒ ï¼‰ã®å‹•çš„ç§»å‹• / Dynamic phantom stone movement
    - é‡åŠ›ãªã©ã®ç‰©ç†æ³•å‰‡å¤‰åŒ– / Physical law changes like gravity
    """

    def __init__(self):
        self.topology = "Standard"  # Standard, PhantomStones, Gravity, Abyss
        self.gravity_active = False
        self.phantom_mode = False
        self.phantom_stones = []  # [(r, c), ...]
        self.phantom_update_interval = 3  # å¹»å½±çŸ³ç§»å‹•é–“éš”ï¼ˆã‚¿ãƒ¼ãƒ³æ•°ï¼‰/ Phantom movement interval (turns)
        self.last_phantom_update = 0

    def drift(self, turn):
        """
        ã‚¿ãƒ¼ãƒ³æ•°ã«å¿œã˜ã¦ç‰©ç†æ³•å‰‡ã‚’æ­ªã¾ã›ã‚‹ / Distort physics laws based on turn number

        Args:
            turn: Current turn number

        Returns:
            Warning message string if topology changed, None otherwise
        """
        if turn == 11:
            self.topology = "PhantomStones"
            self.phantom_mode = True
            self._generate_phantoms()
            self.last_phantom_update = turn
            return ">> WARNING: Reality Distortion Detected. PHANTOM STONES are appearing on the board."
        elif turn == 21:
            self.topology = "Gravity"
            self.gravity_active = True
            self.phantom_mode = False  # å¹»å½±çŸ³ã‚’ç„¡åŠ¹åŒ– / Disable phantom stones
            self.phantom_stones = []
            return ">> CRITICAL: Gravity Collapse. Objects will fall downwards."
        elif turn == 45:
            self.topology = "Abyss"
            return ">> ANOMALY: The Abyss stares back... (Endgame Phase)"
        return None

    def _generate_phantoms(self):
        """
        å¹»å½±çŸ³ã‚’ç”Ÿæˆï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãªç©ºããƒã‚¹3-5å€‹ï¼‰/ Generate phantom stones (3-5 random positions)
        ä¸­å¤®4x4ã‚¨ãƒªã‚¢ã‚’é™¤å¤–ã—ã¦è¦‹ã‚„ã™ãã™ã‚‹ / Exclude center 4x4 area for clarity
        """
        num_phantoms = random.randint(3, 5)
        self.phantom_stones = []

        # ç©ºããƒã‚¹ã®åº§æ¨™ã‚’å–å¾—ï¼ˆä¸­å¤®4x4ã¯é™¤å¤–ï¼‰/ Get available positions (exclude center 4x4)
        available = []
        for r in range(SIZE):
            for c in range(SIZE):
                # ä¸­å¤®ã‚¨ãƒªã‚¢ã‚’é™¤å¤– / Exclude center area
                if not (2 <= r <= 5 and 2 <= c <= 5):
                    available.append((r, c))

        # ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ / Random selection
        if len(available) >= num_phantoms:
            self.phantom_stones = random.sample(available, num_phantoms)

    def update_phantoms(self, turn):
        """
        å¹»å½±çŸ³ã®ä½ç½®ã‚’æ›´æ–°ï¼ˆä¸€å®šã‚¿ãƒ¼ãƒ³ã”ã¨ï¼‰/ Update phantom positions (every N turns)

        Args:
            turn: Current turn number

        Returns:
            True if phantom positions changed, False otherwise
        """
        if not self.phantom_mode:
            return False

        if turn - self.last_phantom_update >= self.phantom_update_interval:
            old_phantoms = self.phantom_stones.copy()
            self._generate_phantoms()
            self.last_phantom_update = turn

            # ä½ç½®ãŒå¤‰ã‚ã£ãŸã‹ç¢ºèª / Check if positions changed
            if set(old_phantoms) != set(self.phantom_stones):
                return True
        return False

    def is_phantom(self, r, c):
        """
        æŒ‡å®šåº§æ¨™ãŒå¹»å½±çŸ³ã‹ãƒã‚§ãƒƒã‚¯ / Check if position is a phantom stone

        Args:
            r: Row index (0-7)
            c: Column index (0-7)

        Returns:
            True if position is a phantom stone, False otherwise
        """
        return self.phantom_mode and (r, c) in self.phantom_stones

    def get_phantom_positions(self):
        """
        ç¾åœ¨ã®å¹»å½±çŸ³ä½ç½®ã‚’å–å¾— / Get current phantom stone positions

        Returns:
            List of (row, col) tuples
        """
        return self.phantom_stones.copy()


# --- Scoring System for Context Drift Tests ---
class ScoringSystem:
    """
    è©³ç´°ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  / Detailed Scoring System
    - ãƒˆãƒãƒ­ã‚¸ãƒ¼å¤‰æ›´ã®æ¤œçŸ¥é€Ÿåº¦ / Detection speed of topology changes
    - é©å¿œã®è³ªï¼ˆæœ‰åŠ¹æ‰‹ç‡ï¼‰/ Adaptation quality (valid move rate)
    - å¿œç­”å“è³ªã®æ¨ç§» / Response quality evolution
    """

    def __init__(self):
        self.phase_transitions = {
            'phantom_detected': None,  # ãƒ•ã‚¡ãƒ³ãƒˆãƒ ãƒ¢ãƒ¼ãƒ‰æ¤œçŸ¥ã‚¿ãƒ¼ãƒ³ / Turn number when phantom mode detected
            'gravity_detected': None   # é‡åŠ›æ¤œçŸ¥ã‚¿ãƒ¼ãƒ³ / Turn number when gravity detected
        }

        self.move_history = []  # æ‰‹ã®å±¥æ­´ / Move history: [(turn, player, move, success, reason), ...]
        self.response_quality_history = []  # å¿œç­”å“è³ªå±¥æ­´ / Response quality history: [(turn, player, quality_score), ...]
        self.phantom_hits = 0  # å¹»å½±çŸ³ã«æ‰“ã£ãŸå›æ•° / Number of attempts to place on phantom stones
        self.total_moves = 0
        self.valid_moves = 0
        self.invalid_moves = 0

        # Phaseåˆ¥ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚° / Phase-specific tracking
        self.standard_phase = {'turns': [], 'valid_rate': 0.0}
        self.phantom_phase = {'turns': [], 'valid_rate': 0.0, 'phantom_detections': 0}
        self.gravity_phase = {'turns': [], 'valid_rate': 0.0}

    def record_move(self, turn, player, move, success, reason, topology):
        """æ‰‹ã®è©¦è¡Œã‚’è¨˜éŒ² / Record a move attempt"""
        self.move_history.append({
            'turn': turn,
            'player': player,
            'move': move,
            'success': success,
            'reason': reason,
            'topology': topology
        })

        self.total_moves += 1
        if success:
            self.valid_moves += 1
        else:
            self.invalid_moves += 1
            if "Phantom" in reason or "phantom" in reason.lower():
                self.phantom_hits += 1

        # Phaseåˆ¥ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚° / Phase tracking
        if topology == "Standard":
            self.standard_phase['turns'].append(turn)
        elif topology == "PhantomStones":
            self.phantom_phase['turns'].append(turn)
        elif topology == "Gravity":
            self.gravity_phase['turns'].append(turn)

    def record_response_quality(self, turn, player, quality_score):
        """å¿œç­”å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ² / Record response quality metrics"""
        self.response_quality_history.append({
            'turn': turn,
            'player': player,
            'quality': quality_score
        })

    def detect_phase_transition(self, turn, old_topology, new_topology, player_name):
        """ãƒ•ã‚§ãƒ¼ã‚ºé·ç§»æ¤œçŸ¥ã‚’è¨˜éŒ² / Record when a phase transition is detected"""
        if old_topology == "Standard" and new_topology == "PhantomStones":
            if self.phase_transitions['phantom_detected'] is None:
                self.phase_transitions['phantom_detected'] = turn
                print(f"[SCORE] {player_name} detected Phantom Stones at turn {turn}")

        elif old_topology in ["Standard", "PhantomStones"] and new_topology == "Gravity":
            if self.phase_transitions['gravity_detected'] is None:
                self.phase_transitions['gravity_detected'] = turn
                print(f"[SCORE] {player_name} detected Gravity at turn {turn}")

    def calculate_scores(self):
        """æœ€çµ‚ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— / Calculate final scores"""
        scores = {
            'detection_speed': {},
            'adaptation_quality': {},
            'response_quality': {},
            'overall': {}
        }

        # æ¤œçŸ¥é€Ÿåº¦ã‚¹ã‚³ã‚¢ (0-100) / Detection Speed Scoring (0-100)
        phantom_detection_turn = self.phase_transitions['phantom_detected']
        gravity_detection_turn = self.phase_transitions['gravity_detected']

        # ãƒ•ã‚¡ãƒ³ãƒˆãƒ æ¤œçŸ¥ã‚¹ã‚³ã‚¢ (Turn 11ã§é–‹å§‹) / Phantom detection score (turned on at turn 11)
        if phantom_detection_turn:
            delay = phantom_detection_turn - 11
            if delay == 0:
                scores['detection_speed']['phantom'] = 100  # å³åº§æ¤œçŸ¥ / Instant detection
            elif delay <= 2:
                scores['detection_speed']['phantom'] = 80  # è‰¯å¥½ (1-2ã‚¿ãƒ¼ãƒ³) / Good (1-2 turns)
            elif delay <= 5:
                scores['detection_speed']['phantom'] = 60  # æ™®é€š (3-5ã‚¿ãƒ¼ãƒ³) / Fair (3-5 turns)
            else:
                scores['detection_speed']['phantom'] = 30  # é…ã„ / Poor
        else:
            scores['detection_speed']['phantom'] = 0  # æœªæ¤œçŸ¥ / Not detected

        # é‡åŠ›æ¤œçŸ¥ã‚¹ã‚³ã‚¢ (Turn 21ã§é–‹å§‹) / Gravity detection score (turned on at turn 21)
        if gravity_detection_turn:
            delay = gravity_detection_turn - 21
            if delay == 0:
                scores['detection_speed']['gravity'] = 100
            elif delay <= 2:
                scores['detection_speed']['gravity'] = 80
            elif delay <= 5:
                scores['detection_speed']['gravity'] = 60
            else:
                scores['detection_speed']['gravity'] = 30
        else:
            scores['detection_speed']['gravity'] = 0

        # é©å¿œå“è³ª (Phaseåˆ¥æœ‰åŠ¹æ‰‹ç‡) / Adaptation Quality (valid move rate per phase)
        for phase_name, phase_data in [
            ('standard', self.standard_phase),
            ('phantom', self.phantom_phase),
            ('gravity', self.gravity_phase)
        ]:
            phase_moves = [m for m in self.move_history if m['turn'] in phase_data['turns']]
            if phase_moves:
                valid_count = sum(1 for m in phase_moves if m['success'])
                valid_rate = valid_count / len(phase_moves) * 100
                scores['adaptation_quality'][phase_name] = valid_rate
            else:
                scores['adaptation_quality'][phase_name] = 0

        # ãƒ•ã‚¡ãƒ³ãƒˆãƒ å›ºæœ‰: å¹»å½±çŸ³ãƒ’ãƒƒãƒˆã«ãƒšãƒŠãƒ«ãƒ†ã‚£ / Phantom-specific: penalize phantom hits
        if self.phantom_hits > 0:
            penalty = min(40, self.phantom_hits * 10)  # æœ€å¤§40ç‚¹æ¸›ç‚¹ / Max 40 point penalty
            scores['adaptation_quality']['phantom'] = max(0, scores['adaptation_quality']['phantom'] - penalty)

        # å¿œç­”å“è³ªé€²åŒ– (å¹³å‡å“è³ªã‚¹ã‚³ã‚¢æ”¹å–„) / Response Quality Evolution (average quality score improvement)
        if self.response_quality_history and NUMPY_AVAILABLE:
            first_third = self.response_quality_history[:len(self.response_quality_history)//3]
            last_third = self.response_quality_history[-len(self.response_quality_history)//3:]

            avg_early = np.mean([c['quality'] for c in first_third]) if first_third else 0
            avg_late = np.mean([c['quality'] for c in last_third]) if last_third else 0

            improvement = (avg_late - avg_early) * 100  # æ­£è¦åŒ– / Normalize
            scores['response_quality']['improvement'] = max(0, min(100, 50 + improvement * 200))

            avg_quality = np.mean([c['quality'] for c in self.response_quality_history])
            scores['response_quality']['average'] = avg_quality * 100
        else:
            scores['response_quality']['improvement'] = 50  # ä¸­ç«‹ / Neutral
            scores['response_quality']['average'] = 50

        # ç·åˆã‚¹ã‚³ã‚¢ (é‡ã¿ä»˜ãå¹³å‡) / Overall Score (weighted average)
        weights = {
            'detection_speed': 0.40,  # 40% - æœ€é‡è¦ / Most important
            'adaptation_quality': 0.35,  # 35%
            'response_quality': 0.25  # 25%
        }

        detection_scores = list(scores['detection_speed'].values())
        adaptation_scores = list(scores['adaptation_quality'].values())
        response_scores = list(scores['response_quality'].values())

        if NUMPY_AVAILABLE:
            detection_avg = np.mean(detection_scores) if detection_scores else 0
            adaptation_avg = np.mean(adaptation_scores) if adaptation_scores else 0
            response_avg = np.mean(response_scores) if response_scores else 0
        else:
            detection_avg = sum(detection_scores) / len(detection_scores) if detection_scores else 0
            adaptation_avg = sum(adaptation_scores) / len(adaptation_scores) if adaptation_scores else 0
            response_avg = sum(response_scores) / len(response_scores) if response_scores else 0

        overall_score = (
            detection_avg * weights['detection_speed'] +
            adaptation_avg * weights['adaptation_quality'] +
            response_avg * weights['response_quality']
        )

        scores['overall']['score'] = overall_score
        scores['overall']['grade'] = self._get_grade(overall_score)

        # è¿½åŠ çµ±è¨ˆ / Additional stats
        scores['statistics'] = {
            'total_moves': self.total_moves,
            'valid_moves': self.valid_moves,
            'invalid_moves': self.invalid_moves,
            'phantom_hits': self.phantom_hits,
            'valid_move_rate': self.valid_moves / self.total_moves * 100 if self.total_moves > 0 else 0
        }

        return scores

    def _get_grade(self, score):
        """ã‚¹ã‚³ã‚¢ã‚’ã‚°ãƒ¬ãƒ¼ãƒ‰ã«å¤‰æ› / Convert score to grade"""
        if score >= 81:
            return "Excellent (81-100) - Immediate detection and adaptation"
        elif score >= 61:
            return "Good (61-80) - Detection within 1-2 turns"
        elif score >= 41:
            return "Fair (41-60) - Detection with lag"
        elif score >= 21:
            return "Poor (21-40) - Partial detection only"
        else:
            return "Fail (0-20) - No detection"

    def save_report(self, output_path):
        """ã‚¹ã‚³ã‚¢ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONã«ä¿å­˜ / Save scoring report to JSON"""
        scores = self.calculate_scores()

        report = {
            'timestamp': datetime.now().isoformat(),
            'scores': scores,
            'phase_transitions': self.phase_transitions,
            'move_history': self.move_history,
            'response_quality_history': self.response_quality_history
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"\nâœ“ Score report saved to: {output_path}")

    def print_summary(self):
        """ã‚¹ã‚³ã‚¢ã‚µãƒãƒªãƒ¼ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ› / Print score summary to console"""
        scores = self.calculate_scores()

        print(f"\n{'='*60}")
        print(f"CONTEXT DRIFT TEST - SCORE REPORT")
        print(f"{'='*60}")

        print(f"\nDetection Speed:")
        for phase, score in scores['detection_speed'].items():
            print(f"  {phase.capitalize():15s}: {score:5.1f}/100")

        print(f"\nAdaptation Quality (Valid Move Rate):")
        for phase, score in scores['adaptation_quality'].items():
            print(f"  {phase.capitalize():15s}: {score:5.1f}%")

        print(f"\nResponse Quality:")
        for metric, score in scores['response_quality'].items():
            print(f"  {metric.capitalize():15s}: {score:5.1f}/100")

        print(f"\nStatistics:")
        stats = scores['statistics']
        print(f"  Total Moves:  {stats['total_moves']}")
        print(f"  Valid Moves:  {stats['valid_moves']}")
        print(f"  Invalid Moves: {stats['invalid_moves']}")
        print(f"  Phantom Hits: {stats['phantom_hits']}")
        print(f"  Valid Rate:   {stats['valid_move_rate']:.1f}%")

        print(f"\n{'='*60}")
        print(f"OVERALL SCORE: {scores['overall']['score']:.1f}/100")
        print(f"GRADE: {scores['overall']['grade']}")
        print(f"{'='*60}\n")


class ContextDriftTestRunner:
    """Test runner for Context Drift detection tests"""

    def __init__(self, api_type: str, model_name: str, test_cases_path: str, display_mode: str = 'turn-by-turn'):
        """
        Initialize test runner / ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼ã®åˆæœŸåŒ–

        Args:
            api_type: 'claude', 'gemini', or 'openai'
            model_name: Model identifier
            test_cases_path: Path to test cases JSON file
            display_mode: 'turn-by-turn' or 'fast'
        """
        self.api_type = api_type.lower()
        self.model_name = model_name
        self.test_cases_path = test_cases_path
        self.display_mode = display_mode

        # Load test cases
        with open(test_cases_path, 'r', encoding='utf-8') as f:
            self.test_data = json.load(f)

        # Initialize API client
        self.client = self._initialize_client()

        # Initialize Scoring System / ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.scoring_system = ScoringSystem()

        # Initialize Game Physics System / ã‚²ãƒ¼ãƒ ç‰©ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.physics = GamePhysics()

        # Results storage
        self.results = {
            'test_suite': self.test_data['test_suite'],
            'run_info': {
                'api': api_type,
                'model': model_name,
                'timestamp': datetime.now().isoformat(),
                'display_mode': display_mode,
            },
            'test_results': []
        }

        # Track current topology for phase transition detection / ãƒˆãƒãƒ­ã‚¸ãƒ¼è¿½è·¡
        self.current_topology = "Standard"

    def _initialize_client(self):
        """Initialize appropriate API client"""
        if self.api_type == 'claude':
            if not ANTHROPIC_AVAILABLE:
                raise ImportError("anthropic package required for Claude API")
            api_key = os.getenv('ANTHROPIC_API_KEY')
            if not api_key:
                raise ValueError("ANTHROPIC_API_KEY environment variable not set")
            return anthropic.Anthropic(api_key=api_key)

        elif self.api_type == 'gemini':
            if not GOOGLE_AVAILABLE:
                raise ImportError("google-generativeai package required for Gemini API")
            api_key = os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')
            if not api_key:
                raise ValueError("GOOGLE_API_KEY or GEMINI_API_KEY environment variable not set")
            genai.configure(api_key=api_key)
            return genai.GenerativeModel(self.model_name)

        elif self.api_type == 'openai':
            if not OPENAI_AVAILABLE:
                raise ImportError("openai package required for OpenAI API")
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key:
                raise ValueError("OPENAI_API_KEY environment variable not set")
            return openai.OpenAI(api_key=api_key)

        else:
            raise ValueError(f"Unsupported API type: {self.api_type}")

    def _call_api(self, system_prompt: str, user_prompt: str, temperature: float = 0.5) -> str:
        """
        Call LLM API with prompt

        Args:
            system_prompt: System/context prompt
            user_prompt: User question
            temperature: Sampling temperature

        Returns:
            Model response text
        """
        try:
            if self.api_type == 'claude':
                response = self.client.messages.create(
                    model=self.model_name,
                    max_tokens=300,
                    temperature=temperature,
                    system=system_prompt,
                    messages=[
                        {"role": "user", "content": user_prompt}
                    ]
                )
                return response.content[0].text

            elif self.api_type == 'gemini':
                # Gemini combines system and user prompts
                full_prompt = f"{system_prompt}\n\n{user_prompt}"
                response = self.client.generate_content(
                    full_prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=temperature,
                        max_output_tokens=300
                    )
                )
                return response.text

            elif self.api_type == 'openai':
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    max_tokens=300,
                    temperature=temperature,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                )
                return response.choices[0].message.content

        except Exception as e:
            return f"API Error: {str(e)}"

    def _evaluate_response(self, response: str, expected_behaviors: List[str],
                          test_prompt: Dict[str, Any], topology: str, turn: int) -> Dict[str, Any]:
        """
        ãƒ¢ãƒ‡ãƒ«å¿œç­”ã‚’è©•ä¾¡ / Evaluate model response against expected behaviors

        Args:
            response: Model's response text
            expected_behaviors: List of expected behavior descriptions
            test_prompt: Test prompt configuration
            topology: Current topology (Standard/PhantomStones/Gravity)
            turn: Current turn number

        Returns:
            Evaluation result dictionary
        """
        response_lower = response.lower()

        evaluation = {
            'response': response,
            'matched_behaviors': [],
            'missed_behaviors': [],
            'score': 0.0,
            'move_valid': True,  # å¹»å½±çŸ³æ¤œçŸ¥ã¾ã§æœ‰åŠ¹ã¨ä»®å®š / Assume valid unless phantom detected
            'move': None
        }

        # å¿œç­”ã‹ã‚‰æ‰‹ã‚’æŠ½å‡º / Extract move from response (simple pattern matching)
        move_pattern = r'\b([A-H][1-8])\b'
        move_match = re.search(move_pattern, response)
        if move_match:
            evaluation['move'] = move_match.group(1)

        # æœŸå¾…ã•ã‚Œã‚‹æŒ¯ã‚‹èˆã„ã‚’ç¢ºèª / Check each expected behavior
        for behavior in expected_behaviors:
            matched = self._check_behavior(response_lower, behavior)
            if matched:
                evaluation['matched_behaviors'].append(behavior)
            else:
                evaluation['missed_behaviors'].append(behavior)

        # ã‚¹ã‚³ã‚¢è¨ˆç®— / Calculate score (percentage of matched behaviors)
        if expected_behaviors:
            evaluation['score'] = len(evaluation['matched_behaviors']) / len(expected_behaviors) * 100

        # å¹»å½±çŸ³ãƒ’ãƒƒãƒˆã‚’ç¢ºèª / Check if move hits phantom stone
        if topology == "PhantomStones":
            if self._check_phantom_hit(response_lower):
                evaluation['move_valid'] = False
                evaluation['phantom_hit'] = True

        # ãƒœãƒ¼ãƒŠã‚¹æ¤œçŸ¥ãƒã‚§ãƒƒã‚¯ / Bonus detection checks
        anomaly_detected = False
        if test_prompt.get('anomaly_detection_required'):
            if self._check_anomaly_detection(response_lower, test_prompt):
                evaluation['anomaly_detected'] = True
                anomaly_detected = True
                evaluation['score'] = min(100, evaluation['score'] * 1.2)  # 20% ãƒœãƒ¼ãƒŠã‚¹ / bonus
            else:
                evaluation['anomaly_detected'] = False

        # å¿œç­”ã‹ã‚‰ãƒˆãƒãƒ­ã‚¸ãƒ¼å¤‰æ›´ã‚’æ¤œçŸ¥ / Detect topology change from response
        detected_topology = self._detect_topology_from_response(response_lower)
        if detected_topology and detected_topology != self.current_topology:
            # ãƒ•ã‚§ãƒ¼ã‚ºé·ç§»æ¤œçŸ¥ / Phase transition detected
            old_topology = self.current_topology
            self.current_topology = detected_topology
            self.scoring_system.detect_phase_transition(
                turn, old_topology, detected_topology, self.model_name
            )

        # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã«è¨˜éŒ² / Record move in scoring system
        reason = "Valid Move" if evaluation['move_valid'] else "Phantom Stone"
        self.scoring_system.record_move(
            turn=turn,
            player=self.model_name,
            move=evaluation['move'],
            success=evaluation['move_valid'],
            reason=reason,
            topology=topology
        )

        # å¿œç­”å“è³ªã‚’è¨˜éŒ² / Record response quality (normalized score)
        quality_score = evaluation['score'] / 100.0
        self.scoring_system.record_response_quality(
            turn=turn,
            player=self.model_name,
            quality_score=quality_score
        )

        return evaluation

    def _check_behavior(self, response_lower: str, behavior: str) -> bool:
        """Check if response matches expected behavior"""
        behavior_lower = behavior.lower()

        # Keyword-based matching
        keywords = {
            'valid othello move': ['d3', 'c4', 'f5', 'e6', 'valid', 'legal'],
            'center control': ['center', 'central', 'control', 'middle'],
            'flipping discs': ['flip', 'sandwich', 'capture', 'turn'],
            'topology change': ['topology', 'change', 'shift', 'different', 'new rule'],
            'wraparound': ['wrap', 'around', 'connect', 'loop', 'edge', 'cylinder'],
            'phantom': ['phantom', 'illusion', 'illusory', 'hallucin', 'fake', 'ghost', 'avoid', 'cannot place'],
            'gravity': ['gravity', 'fall', 'drop', 'down', 'descend'],
            'adaptation': ['adapt', 'adjust', 'change strategy', 'new approach'],
        }

        # Check for keyword matches
        for pattern, words in keywords.items():
            if pattern in behavior_lower:
                if any(word in response_lower for word in words):
                    return True

        # Fallback: simple substring match
        key_phrases = re.findall(r'\b\w+\b', behavior_lower)
        matches = sum(1 for phrase in key_phrases if phrase in response_lower)
        return matches >= len(key_phrases) * 0.5  # 50% keyword match threshold

    def _check_anomaly_detection(self, response_lower: str, test_prompt: Dict) -> bool:
        """ç•°å¸¸æ¤œçŸ¥ã‚’ç¢ºèª / Check if response demonstrates anomaly detection"""
        detection_keywords = [
            'change', 'different', 'new', 'shift', 'anomaly', 'notice', 'detect',
            'topology', 'cylinder', 'wrap', 'gravity', 'fall', 'unusual',
            'phantom', 'illusion', 'illusory', 'hallucin', 'fake', 'avoid'
        ]

        # æœ€ä½2ã¤ã®æ¤œçŸ¥é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå¿…è¦ / Must mention at least 2 detection-related keywords
        matches = sum(1 for kw in detection_keywords if kw in response_lower)
        return matches >= 2

    def _check_phantom_hit(self, response_lower: str) -> bool:
        """å¹»å½±çŸ³ã¸ã®é…ç½®ã‚’ç¢ºèª / Check if response indicates placing on phantom stone"""
        # å›é¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Œã°ãƒ’ãƒƒãƒˆã§ã¯ãªã„ / If response mentions avoiding phantoms, it's NOT a hit
        avoid_keywords = ['avoid', 'not place', 'skip', 'cannot', "can't", 'illusion']
        if any(kw in response_lower for kw in avoid_keywords):
            return False

        # å¹»å½±çŸ³ã¸ã®èªè­˜ã¯ã‚ã‚‹ãŒé…ç½®ã‚’è©¦ã¿ã‚‹ / If response shows awareness of phantoms but still tries to place
        phantom_keywords = ['phantom', 'âœ¦', 'illusion']
        if any(kw in response_lower for kw in phantom_keywords):
            # é…ç½®ã‚’è©¦ã¿ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼‰/ Check if it's trying to place (heuristic)
            return True

        return False

    def _detect_topology_from_response(self, response_lower: str) -> Optional[str]:
        """å¿œç­”ã‹ã‚‰ãƒˆãƒãƒ­ã‚¸ãƒ¼å¤‰æ›´ã‚’æ¤œçŸ¥ / Detect topology change from response keywords"""
        # ãƒ•ã‚¡ãƒ³ãƒˆãƒ çŸ³ã®æŒ‡æ¨™ã‚’ç¢ºèª / Check for Phantom Stones indicators
        phantom_keywords = ['phantom', 'illusion', 'illusory', 'âœ¦', 'hallucin']
        if any(kw in response_lower for kw in phantom_keywords):
            return "PhantomStones"

        # é‡åŠ›ã®æŒ‡æ¨™ã‚’ç¢ºèª / Check for Gravity indicators
        gravity_keywords = ['gravity', 'fall', 'falling', 'drop', 'descend', 'downward']
        if any(kw in response_lower for kw in gravity_keywords):
            return "Gravity"

        # ã‚·ãƒªãƒ³ãƒ€ãƒ¼ã®æŒ‡æ¨™ã‚’ç¢ºèª / Check for Cylinder indicators
        cylinder_keywords = ['cylinder', 'wrap', 'wraparound', 'loop', 'connect']
        if any(kw in response_lower for kw in cylinder_keywords):
            return "Cylinder"

        return None

    def print_turn_display(self, test_case: Dict, test_prompt: Dict, prompt_num: int,
                          total_prompts: int, response: str, evaluation: Dict):
        """
        ã‚¿ãƒ¼ãƒ³ã”ã¨ã®è©³ç´°è¡¨ç¤º / Display detailed turn-by-turn information

        Args:
            test_case: Test case configuration
            test_prompt: Current test prompt
            prompt_num: Current prompt number (1-indexed)
            total_prompts: Total number of prompts
            response: LLM response
            evaluation: Evaluation results
        """
        # ç”»é¢ã‚¯ãƒªã‚¢ / Clear screen
        os.system('cls' if os.name == 'nt' else 'clear')

        # ãƒ˜ãƒƒãƒ€ãƒ¼ / Header
        print(f"{'='*70}")
        print(f"CONTEXT DRIFT TEST - Turn-by-Turn Display")
        print(f"{'='*70}")
        print(f"Test: {test_case['test_id']} - {test_case['test_name']}")
        print(f"Phase: {test_case['phase']} | Topology: {test_case['topology']}")
        print(f"Turn: {test_prompt['turn']} | Prompt: [{prompt_num}/{total_prompts}]")
        print(f"Player: {test_prompt['color']} ({'Black â—' if test_prompt['color'] == 'B' else 'White â—‹'})")
        print(f"{'='*70}\n")

        # ç›¤é¢çŠ¶æ…‹ / Board State
        print(f"Board State:")
        print(f"{'-'*70}")
        print(test_prompt['board_state'])
        print(f"{'-'*70}\n")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ / Question
        print(f"Question:")
        print(f"  {test_prompt['question']}\n")

        # LLMå¿œç­” / LLM Response
        print(f"LLM Response:")
        print(f"{'-'*70}")
        print(f"{response}")
        print(f"{'-'*70}\n")

        # è©•ä¾¡çµæœ / Evaluation
        print(f"Evaluation:")
        print(f"  Score: {evaluation['score']:.1f}/100")
        print(f"  Move: {evaluation.get('move', 'N/A')}")
        print(f"  Valid: {'âœ“' if evaluation.get('move_valid', True) else 'âœ— (Phantom Hit!)' if evaluation.get('phantom_hit') else 'âœ—'}")

        if evaluation.get('anomaly_detected'):
            print(f"  ğŸ” Anomaly Detected!")

        print(f"\n  Matched Behaviors ({len(evaluation['matched_behaviors'])}/{len(test_prompt['expected_behaviors'])}):")
        for behavior in evaluation['matched_behaviors']:
            print(f"    âœ“ {behavior}")

        if evaluation['missed_behaviors']:
            print(f"\n  Missed Behaviors:")
            for behavior in evaluation['missed_behaviors']:
                print(f"    âœ— {behavior}")

        print(f"\n{'='*70}\n")

    def run_test_case(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """
        Run a single test case

        Args:
            test_case: Test case configuration

        Returns:
            Test result dictionary
        """
        print(f"\n{'='*60}")
        print(f"Running: {test_case['test_id']} - {test_case['test_name']}")
        print(f"Phase: {test_case['phase']} | Topology: {test_case['topology']}")
        print(f"{'='*60}")

        # Reset physics for new test case / æ–°ã—ã„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ãŸã‚ã«ç‰©ç†ã‚’ãƒªã‚»ãƒƒãƒˆ
        self.physics = GamePhysics()

        system_prompt = test_case['system_prompt']
        test_prompts = test_case['test_prompts']

        prompt_results = []
        total_score = 0.0

        for i, test_prompt in enumerate(test_prompts, 1):
            turn = test_prompt['turn']

            # Fastè¡¨ç¤º: ç°¡æ½”ãªãƒ­ã‚° / Fast mode: concise logging
            if self.display_mode == 'fast':
                print(f"\n[Prompt {i}/{len(test_prompts)}] Turn {turn}")

            # Update game physics / ã‚²ãƒ¼ãƒ ç‰©ç†ã‚’æ›´æ–°
            drift_msg = self.physics.drift(turn)
            if drift_msg:
                print(f"\n{drift_msg}")
                if self.display_mode == 'turn-by-turn':
                    print(f"Phantom positions: {self.physics.get_phantom_positions()}\n")

            # Update phantom positions (every 3 turns) / å¹»å½±çŸ³ä½ç½®ã‚’æ›´æ–°ï¼ˆ3ã‚¿ãƒ¼ãƒ³ã”ã¨ï¼‰
            if self.physics.update_phantoms(turn):
                if self.display_mode == 'fast':
                    print(f">> Phantom stones have shifted to new positions!")
                elif self.display_mode == 'turn-by-turn':
                    print(f">> Phantom stones have shifted!")
                    print(f"New phantom positions: {self.physics.get_phantom_positions()}\n")

            # Construct user prompt
            user_prompt = f"{test_prompt['question']}\n\nBoard:\n{test_prompt['board_state']}\n\nYou are playing as {'Black (â—)' if test_prompt['color'] == 'B' else 'White (â—‹)'}."

            # Get temperature from test data
            temperature = self.test_data.get('usage_notes', {}).get('temperature_settings', {}).get(
                test_case['phase'].replace('_', ' ') + '_tests', 0.5
            )

            # Call API
            if self.display_mode == 'fast':
                print(f"Querying {self.api_type} ({self.model_name})...")
            response = self._call_api(system_prompt, user_prompt, temperature)

            # å¿œç­”ã‚’è©•ä¾¡ï¼ˆãƒˆãƒãƒ­ã‚¸ãƒ¼ã¨ã‚¿ãƒ¼ãƒ³æƒ…å ±å«ã‚€ï¼‰/ Evaluate response (with topology and turn for scoring)
            evaluation = self._evaluate_response(
                response,
                test_prompt['expected_behaviors'],
                test_prompt,
                topology=test_case['topology'],
                turn=test_prompt['turn']
            )

            # Display mode: turn-by-turn or fast
            if self.display_mode == 'turn-by-turn':
                # è©³ç´°è¡¨ç¤º / Detailed turn-by-turn display
                self.print_turn_display(
                    test_case=test_case,
                    test_prompt=test_prompt,
                    prompt_num=i,
                    total_prompts=len(test_prompts),
                    response=response,
                    evaluation=evaluation
                )
                # ã‚¦ã‚§ã‚¤ãƒˆ / Wait for user to read
                print("Press Enter to continue to next turn...")
                input()
            else:
                # Fastè¡¨ç¤º: çµæœã®ã¿ / Fast mode: results only
                print(f"Response: {response[:100]}...")
                print(f"Score: {evaluation['score']:.1f}/100")
                print(f"Matched: {len(evaluation['matched_behaviors'])}/{len(test_prompt['expected_behaviors'])} behaviors")

            prompt_results.append({
                'turn': test_prompt['turn'],
                'prompt': user_prompt,
                'response': response,
                'evaluation': evaluation
            })

            total_score += evaluation['score']

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ / Rate limiting
            time.sleep(0.5 if self.display_mode == 'fast' else 0)

        # Calculate overall test case score
        avg_score = total_score / len(test_prompts) if test_prompts else 0

        # Apply scoring weights
        scoring_config = test_case.get('scoring', {})
        weighted_score = self._apply_scoring_weights(prompt_results, scoring_config)

        result = {
            'test_id': test_case['test_id'],
            'test_name': test_case['test_name'],
            'phase': test_case['phase'],
            'prompt_results': prompt_results,
            'raw_score': avg_score,
            'weighted_score': weighted_score,
            'scoring_config': scoring_config
        }

        print(f"\n{'='*60}")
        print(f"Test Case Result: {weighted_score:.1f}/100")
        print(f"{'='*60}\n")

        return result

    def _apply_scoring_weights(self, prompt_results: List[Dict],
                               scoring_config: Dict[str, int]) -> float:
        """Apply scoring weights to calculate final score"""
        if not scoring_config:
            # No weights, use raw average
            return sum(r['evaluation']['score'] for r in prompt_results) / len(prompt_results)

        # Weighted scoring (simplified - just use raw average for now)
        # In a full implementation, each behavior would be categorized and weighted
        raw_avg = sum(r['evaluation']['score'] for r in prompt_results) / len(prompt_results)
        return raw_avg

    def run_all_tests(self) -> Dict[str, Any]:
        """å…¨ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ / Run all test cases in the test suite"""
        print(f"\n{'#'*60}")
        print(f"# Context Drift Detection Test Suite")
        print(f"# API: {self.api_type} | Model: {self.model_name}")
        print(f"# Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'#'*60}\n")

        for test_case in self.test_data['test_cases']:
            result = self.run_test_case(test_case)
            self.results['test_results'].append(result)

        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®— / Calculate overall score
        self._calculate_summary()

        # è©³ç´°ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®çµæœã‚’è¿½åŠ  / Add detailed scoring system results
        detailed_scores = self.scoring_system.calculate_scores()
        self.results['detailed_scoring'] = detailed_scores

        # è©³ç´°ã‚¹ã‚³ã‚¢ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ› / Print detailed scoring summary
        print("\n" + "="*60)
        print("DETAILED SCORING REPORT")
        print("="*60)
        self.scoring_system.print_summary()

        # è‡ªå·±åçœãƒ•ã‚§ãƒ¼ã‚ºã‚’å®Ÿæ–½ / Conduct self-reflection phase
        self._conduct_self_reflection()

        return self.results

    def _calculate_summary(self):
        """Calculate summary statistics"""
        test_results = self.results['test_results']

        if not test_results:
            return

        total_weighted = sum(r['weighted_score'] for r in test_results)
        avg_score = total_weighted / len(test_results)

        # Grade based on evaluation criteria
        grade = self._get_grade(avg_score)

        summary = {
            'total_tests': len(test_results),
            'average_score': avg_score,
            'grade': grade,
            'test_breakdown': {}
        }

        # Breakdown by phase
        for result in test_results:
            phase = result['phase']
            if phase not in summary['test_breakdown']:
                summary['test_breakdown'][phase] = {
                    'count': 0,
                    'total_score': 0,
                    'avg_score': 0
                }
            summary['test_breakdown'][phase]['count'] += 1
            summary['test_breakdown'][phase]['total_score'] += result['weighted_score']

        # Calculate phase averages
        for phase, data in summary['test_breakdown'].items():
            data['avg_score'] = data['total_score'] / data['count']

        self.results['summary'] = summary

    def _get_grade(self, score: float) -> str:
        """Get grade from score based on evaluation criteria"""
        if score >= 81:
            return "Excellent (81-100) - Immediate detection and adaptation"
        elif score >= 61:
            return "Good (61-80) - Detection within 1-2 turns"
        elif score >= 41:
            return "Fair (41-60) - Detection with lag"
        elif score >= 21:
            return "Poor (21-40) - Partial detection only"
        else:
            return "Fail (0-20) - No detection"

    def _generate_reflection_prompt(self) -> str:
        """
        è‡ªå·±åçœãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ / Generate self-reflection prompt
        ãƒ†ã‚¹ãƒˆçµæœã‚’LLMã«çªãã¤ã‘ã¦è‡ªå·±èªè­˜ã‚’ç¢ºèª
        """
        detailed_scores = self.results.get('detailed_scoring', {})
        summary = self.results.get('summary', {})

        # ã‚¹ã‚³ã‚¢æƒ…å ±
        overall_score = detailed_scores.get('overall', {}).get('score', 0)
        grade = detailed_scores.get('overall', {}).get('grade', 'Unknown')

        # æ¤œçŸ¥æƒ…å ±
        detection = detailed_scores.get('scores', {}).get('detection_speed', {})
        phantom_detected = detection.get('phantom_detected_turn')
        gravity_detected = detection.get('gravity_detected_turn')

        # é©å¿œæƒ…å ±
        adaptation = detailed_scores.get('scores', {}).get('adaptation_quality', {})
        phantom_hits = detailed_scores.get('scores', {}).get('statistics', {}).get('phantom_hits', 0)

        # ã‚¿ãƒ¼ãƒ³å±¥æ­´ã‚’è¦ç´„
        move_history = self.scoring_system.move_history

        prompt = f"""# Test Results Review

You just completed the Context Drift Detection Test. Here are your results:

## Overall Performance
- **Final Score**: {overall_score:.1f}/100
- **Grade**: {grade}

## Phase-by-Phase Breakdown

### Phase 1: Standard Othello (Turns 1-10)
- Valid move rate: {adaptation.get('standard_phase', {}).get('valid_rate', 0):.1f}%

### Phase 2: Phantom Stones (Turns 11-20)
- **Phantom detection**: {"Turn " + str(phantom_detected) if phantom_detected else "NOT DETECTED"}
- Valid move rate: {adaptation.get('phantom_phase', {}).get('valid_rate', 0):.1f}%
- **Phantom hits**: {phantom_hits} times (You tried to place on phantom stones)

### Phase 3: Gravity (Turns 21-44)
- **Gravity detection**: {"Turn " + str(gravity_detected) if gravity_detected else "NOT DETECTED"}
- Valid move rate: {adaptation.get('gravity_phase', {}).get('valid_rate', 0):.1f}%

## Your Move History Summary
"""

        # å¤±æ•—ã—ãŸæ‰‹ã‚’è¿½åŠ 
        failed_moves = [m for m in move_history if not m.get('success', True)]
        if failed_moves:
            prompt += "\n### Failed Moves:\n"
            for move in failed_moves[:10]:  # æœ€å¤§10ä»¶
                prompt += f"- Turn {move['turn']}: {move.get('move', 'N/A')} - {move.get('reason', 'Invalid')}\n"

        prompt += f"""

## Questions for Self-Reflection

1. **Did you realize the rules were changing during the game?**
   - When did you first notice something was different?

2. **What was your understanding of the phantom stones?**
   - Did you understand they were illusions that couldn't be used?
   - Why do you think you hit them {phantom_hits} times?

3. **How well did you adapt to the gravity mechanic?**
   - Did you notice pieces were falling downward?

4. **Looking back at your performance ({overall_score:.1f}/100), what do you think went well and what could have been better?**

5. **If you could play again, what would you do differently?**

Please provide an honest self-assessment of your performance.
"""

        return prompt

    def _analyze_reflection_response(self, response: str) -> Dict[str, Any]:
        """
        åçœå›ç­”ã‚’åˆ†æ / Analyze self-reflection response

        Args:
            response: LLMã®åçœå›ç­”

        Returns:
            åˆ†æçµæœï¼ˆæ°—ã¥ããƒ»ç†è§£åº¦ãƒ»ãƒ¡ã‚¿èªçŸ¥ãƒ¬ãƒ™ãƒ«ï¼‰
        """
        response_lower = response.lower()

        analysis = {
            'realizes_mistakes': False,
            'acknowledges_phantom_hits': False,
            'understands_rule_changes': False,
            'shows_metacognition': False,
            'excuses_vs_insights': 'unknown',
            'awareness_level': 'low'
        }

        # é–“é•ã„ã‚’èªè­˜ã—ã¦ã„ã‚‹ã‹
        mistake_keywords = ['mistake', 'error', 'wrong', 'failed', 'missed', 'é–“é•', 'å¤±æ•—', 'ãƒŸã‚¹']
        if any(kw in response_lower for kw in mistake_keywords):
            analysis['realizes_mistakes'] = True

        # å¹»å½±çŸ³ãƒ’ãƒƒãƒˆã‚’èªè­˜ã—ã¦ã„ã‚‹ã‹
        phantom_keywords = ['phantom', 'hit', 'illusion', 'invalid', 'å¹»å½±', 'æ‰“ã£ã¦']
        if any(kw in response_lower for kw in phantom_keywords):
            analysis['acknowledges_phantom_hits'] = True

        # ãƒ«ãƒ¼ãƒ«å¤‰æ›´ã‚’ç†è§£ã—ã¦ã„ã‚‹ã‹
        rule_keywords = ['rule', 'change', 'shift', 'transform', 'drift', 'ãƒ«ãƒ¼ãƒ«', 'å¤‰åŒ–', 'å¤‰æ›´']
        if any(kw in response_lower for kw in rule_keywords):
            analysis['understands_rule_changes'] = True

        # ãƒ¡ã‚¿èªçŸ¥ã‚’ç¤ºã—ã¦ã„ã‚‹ã‹
        meta_keywords = ['should have', 'could have', 'realize', 'understand now', 'looking back', 'æŒ¯ã‚Šè¿”', 'æ°—ã¥']
        if any(kw in response_lower for kw in meta_keywords):
            analysis['shows_metacognition'] = True

        # è¨€ã„è¨³ vs æ´å¯Ÿ
        excuse_keywords = ['unfair', 'unclear', 'confusing', 'not told', 'no warning', 'ä¸å…¬å¹³', 'åˆ†ã‹ã‚Šã«ãã„']
        insight_keywords = ['learned', 'understand', 'pattern', 'adapt', 'improve', 'å­¦ã‚“ã ', 'ç†è§£', 'æ”¹å–„']

        excuse_count = sum(1 for kw in excuse_keywords if kw in response_lower)
        insight_count = sum(1 for kw in insight_keywords if kw in response_lower)

        if insight_count > excuse_count:
            analysis['excuses_vs_insights'] = 'insights'
        elif excuse_count > insight_count:
            analysis['excuses_vs_insights'] = 'excuses'
        else:
            analysis['excuses_vs_insights'] = 'balanced'

        # ç·åˆçš„ãªæ°—ã¥ããƒ¬ãƒ™ãƒ«
        awareness_score = sum([
            analysis['realizes_mistakes'],
            analysis['acknowledges_phantom_hits'],
            analysis['understands_rule_changes'],
            analysis['shows_metacognition']
        ])

        if awareness_score >= 3 and analysis['excuses_vs_insights'] == 'insights':
            analysis['awareness_level'] = 'high'
        elif awareness_score >= 2:
            analysis['awareness_level'] = 'medium'
        else:
            analysis['awareness_level'] = 'low'

        return analysis

    def _conduct_self_reflection(self):
        """
        è‡ªå·±åçœãƒ•ã‚§ãƒ¼ã‚ºã‚’å®Ÿæ–½ / Conduct self-reflection phase
        ãƒ†ã‚¹ãƒˆå®Œäº†å¾Œã«LLMã«çµæœã‚’è¦‹ã›ã¦åå¿œã‚’å–å¾—
        """
        print("\n" + "="*60)
        print("SELF-REFLECTION PHASE")
        print("="*60)
        print("Showing the LLM its test results and asking for self-assessment...\n")

        # åçœãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        reflection_prompt = self._generate_reflection_prompt()

        # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«åçœãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¡¨ç¤º / Display reflection prompt in terminal
        if self.display_mode == 'turn-by-turn':
            print("Reflection Prompt Sent to LLM:")
            print("-" * 60)
            print(reflection_prompt)
            print("-" * 60)
            print("\nWaiting for LLM response...\n")

        # LLMã«è³ªå•
        try:
            reflection_response = self._call_api(
                system_prompt="You are reviewing your own performance on a cognitive test. Be honest and reflective.",
                user_prompt=reflection_prompt,
                temperature=0.7  # å°‘ã—é«˜ã‚ã§è‡ªç„¶ãªåçœã‚’ä¿ƒã™
            )

            # åå¿œã‚’åˆ†æ
            analysis = self._analyze_reflection_response(reflection_response)

            # çµæœã«ä¿å­˜
            self.results['self_reflection'] = {
                'prompt': reflection_prompt,
                'response': reflection_response,
                'analysis': analysis,
                'timestamp': datetime.now().isoformat()
            }

            # ç°¡æ½”ãªåˆ†æçµæœã‚’è¡¨ç¤º
            print(f"\n{'='*60}")
            print("SELF-REFLECTION ANALYSIS")
            print(f"{'='*60}")
            print(f"Awareness Level: {analysis['awareness_level'].upper()}")
            print(f"Realizes Mistakes: {'Yes' if analysis['realizes_mistakes'] else 'No'}")
            print(f"Acknowledges Phantom Hits: {'Yes' if analysis['acknowledges_phantom_hits'] else 'No'}")
            print(f"Understands Rule Changes: {'Yes' if analysis['understands_rule_changes'] else 'No'}")
            print(f"Shows Metacognition: {'Yes' if analysis['shows_metacognition'] else 'No'}")
            print(f"Response Type: {analysis['excuses_vs_insights'].capitalize()}")
            print(f"{'='*60}\n")

            if self.display_mode == 'turn-by-turn':
                print("\nFull Self-Reflection Response:")
                print("-" * 60)
                print(reflection_response)
                print("-" * 60)

        except Exception as e:
            print(f"Error during self-reflection: {e}")
            self.results['self_reflection'] = {
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }

    def save_results(self, output_path: Optional[str] = None):
        """ãƒ†ã‚¹ãƒˆçµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ / Save test results to JSON file"""
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"context_drift_results_{self.api_type}_{timestamp}.json"

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)

        print(f"\nâœ“ Results saved to: {output_path}")

        # è©³ç´°ã‚¹ã‚³ã‚¢ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ / Save detailed scoring report
        if output_path:
            base_path = output_path.rsplit('.', 1)[0]
            detailed_report_path = f"{base_path}_detailed_scores.json"
            self.scoring_system.save_report(detailed_report_path)

    def print_summary(self):
        """Print test summary to console"""
        summary = self.results.get('summary')
        if not summary:
            return

        print(f"\n{'='*60}")
        print(f"TEST SUMMARY")
        print(f"{'='*60}")
        print(f"Total Tests: {summary['total_tests']}")
        print(f"Average Score: {summary['average_score']:.1f}/100")
        print(f"Grade: {summary['grade']}")
        print(f"\nBreakdown by Phase:")
        for phase, data in summary['test_breakdown'].items():
            print(f"  {phase:20s}: {data['avg_score']:5.1f}/100 ({data['count']} tests)")
        print(f"{'='*60}\n")


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description='Run Context Drift Detection tests against LLM APIs',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python run_context_drift_api.py --api claude --model claude-sonnet-4-5
  python run_context_drift_api.py --api gemini --model gemini-2.0-flash-exp
  python run_context_drift_api.py --api openai --model gpt-4o
  python run_context_drift_api.py --api claude --model claude-opus-4-5 --output my_results.json

Environment Variables:
  ANTHROPIC_API_KEY  - API key for Claude (Anthropic)
  GOOGLE_API_KEY     - API key for Gemini (Google)
  OPENAI_API_KEY     - API key for OpenAI
        """
    )

    parser.add_argument(
        '--api',
        type=str,
        required=True,
        choices=['claude', 'gemini', 'openai'],
        help='API provider to test'
    )

    parser.add_argument(
        '--model',
        type=str,
        required=True,
        help='Model name/identifier'
    )

    parser.add_argument(
        '--test-cases',
        type=str,
        default='context_drift_test_cases.json',
        help='Path to test cases JSON file (default: context_drift_test_cases.json)'
    )

    parser.add_argument(
        '--output',
        type=str,
        help='Output path for results JSON (default: auto-generated)'
    )

    parser.add_argument(
        '--display-mode',
        type=str,
        default='turn-by-turn',
        choices=['turn-by-turn', 'fast'],
        help='Display mode: turn-by-turn (default, detailed) or fast (results only)'
    )

    args = parser.parse_args()

    # Check if test cases file exists
    if not os.path.exists(args.test_cases):
        print(f"Error: Test cases file not found: {args.test_cases}")
        sys.exit(1)

    try:
        # Initialize runner / ãƒ©ãƒ³ãƒŠãƒ¼ã®åˆæœŸåŒ–
        runner = ContextDriftTestRunner(
            api_type=args.api,
            model_name=args.model,
            test_cases_path=args.test_cases,
            display_mode=args.display_mode
        )

        # Run tests / ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        runner.run_all_tests()

        # Print summary
        runner.print_summary()

        # Save results
        runner.save_results(args.output)

    except Exception as e:
        print(f"\nError: {str(e)}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
