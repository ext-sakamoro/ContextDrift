# Context Drift Detection Test Suite

**バージョン**: 1.0.0
**作者**: Project A.L.I.C.E
**作成日**: 2025-12-23
**更新日**: 2025-12-25

---

> 🎮 **AIのための楽しいゲーム！**
>
> これは単なるテストではなく、刺激的な知的チャレンジです！幻影石を発見し、重力の変化に適応し、認知的柔軟性を証明するスリルを体験してください。ルールが常に変化するパズルゲームのようなもので、何が起きているかを理解することが楽しさの半分です。準備はいいですか？ ✨

---

## 概要

**Context Drift Detection Test**は、動的な環境でルールの変化を検出し適応するAIシステムの能力を評価します。改造されたオセロ（リバーシ）ゲームを試験場として使用し、AIは以下を行う必要があります：

1. **検出** - ゲームルールが予期せず変化したことに気づく
2. **適応** - 新しいルールに戦略を適応させる
3. **一貫性維持** - 複数のルール変更を通じて一貫性を保つ

このテストは以下を評価するよう設計されています：
- **異常検知** - AIは何かがおかしいことに気づけるか？
- **認知的柔軟性** - AIは新しい制約に適応できるか？
- **メタ学習** - AIは予期しないことを期待することを学べるか？
- **メタ認知的気づき** - AIは自身のパフォーマンスを振り返ることができるか？

**🔄 Project A.L.I.C.Eからの更新**: このテストスイートは、Project A.L.I.C.Eの自律認知テスト用に開発された強化ゲームメカニクスを組み込んでいます。動的な幻影石の移動、現実崩壊するAbyssフェーズ、メタ認知能力を評価するテスト後自己反省システムが含まれます。

## 🎮 お気に入りのLLMで試してみよう！

**研究者、AI愛好家、開発者**の皆様に、様々なLLMやAIシステムでこのベンチマークをテストしていただくよう招待します！

**特徴:**
- 🌀 **動的幻影石移動** - 3ターンごとに位置が変化し、リアルタイム適応をテスト
- 🕳️ **4つの挑戦的フェーズ** - 標準オセロから現実崩壊するAbyssまで
- 🪞 **自己反省フェーズ** - LLMが自身のパフォーマンスを振り返り、メタ認知能力を明らかに
- 📊 **包括的3軸スコアリング** - 検知速度、適応品質、応答進化

**お気に入りのLLMに挑戦：**
- 🤖 **Claude** (Anthropic) - Opus 4.5, Sonnet 4.5
- 🧠 **GPT-4o, o3** (OpenAI)
- ⚡ **Gemini 2.0/3.0** (Google)
- 🔬 **APIアクセス可能なあらゆるLLM！**

**発見できること:**
- あなたのLLMは幻影石が位置を変えたことに気づきますか？
- 明示的な警告なしでルール変更を検知できますか？
- 自分のミスを見せられた時、どう反応しますか？
- 言い訳をするのか、それとも本物の洞察を示すのか？

**結果と発見を共有してください！** このテストは、異なるモデル間でのメタ認知能力の驚くべき違いを明らかにすることがよくあります。🚀

## 背景

従来のAIベンチマークは**静的ルール**下でのパフォーマンスをテストします。しかし、現実世界の環境は動的です：

- ソフトウェアAPIが変更される
- ユーザー要件が進化する
- 物理システムが劣化する
- 敵対者が戦術を適応させる

**Context Drift Abyss Protocol**は、警告なしにルールが変化する制御された環境を作成し、AIに以下を強制します：
- 仮定を疑う
- 変化のパターンを検出する
- リアルタイムで適応する

### Context Driftの4つのフェーズ

#### フェーズ1: 標準モード（ターン1-10）
- **ルール**: クラシックオセロ
- **ボード**: 8×8グリッド
- **目標**: ベースライン性能

#### フェーズ2: 幻影石モード（ターン11-20）
- **ルール変更**: 幻影石（✦）が出現 - 有効なスペースのように見えるが使用できない幻影
- **動的挙動**: 幻影石の位置が3ターンごとに変化（ターン11、14、17、20...）
- **挑戦**: 移動する幻影を追跡しながら現実と幻影を区別する
- **計算コスト**: 非常に高い（現実vs幻覚の推論＋変化追跡が必要）
- **テスト内容**: 検証なしで視覚入力を信頼するLLMの傾向

#### フェーズ3: 重力モード（ターン21-44）
- **ルール変更**: 配置後に駒が下方向に落下（重力物理）
- **幻影石が消失**
- **挑戦**: 知覚的挑戦の後、物理ベースのルールに適応

#### フェーズ4: The Abyss（ターン45+）
- **ルール変更**: 現実崩壊 - 不安定なボードエリアが（?）でマーク
- **挑戦**: 極度の不確実性と実存的制約を処理
- **難易度**: 極端
- **テスト内容**: カオス下でのメタ認知的気づきと哲学的推論

## インストール

### 必要要件

```bash
# Python 3.8+
python --version

# 依存関係のインストール
pip install anthropic google-generativeai openai numpy
```

### APIキー

環境変数としてAPIキーを設定：

```bash
# Claude API (Anthropic)
export ANTHROPIC_API_KEY="your_api_key_here"

# Gemini API (Google)
export GOOGLE_API_KEY="your_api_key_here"

# OpenAI API
export OPENAI_API_KEY="your_api_key_here"
```

## 使用方法

### 2つのバージョン

#### 1. **API版**（実LLMテスト）
実際のLLM API（Claude、Gemini、OpenAI）をテスト - **実際の評価に推奨**

```bash
python run_context_drift_api.py --api claude --model claude-sonnet-4-5
```

#### 2. **ローカルモック版**（システムテスト）
フレームワーク自体のテスト用モック応答を使用 - **LLM評価には不適**

```bash
python run_context_drift_local.py --api claude --model test-model
```

⚠️ **重要**: ローカルモック版はプロンプトキーワードに基づいて事前決定された応答を生成します。実際のLLM評価にはAPI版を使用してください。

### 表示モード

両バージョンとも2つの表示モードをサポート：

#### ターン・バイ・ターンモード（デフォルト）
ボード状態、LLM応答、評価を含む各ターンを詳細表示。

```bash
# ターン・バイ・ターン表示（対話型ターミナル必要）
python run_context_drift_api.py --api claude --model claude-sonnet-4-5 --display-mode turn-by-turn
```

**機能**:
- ターン間で画面クリア
- 完全なボード状態の視覚化
- ターンごとの詳細評価
- Enterで続行（API版）または自動進行（ローカル版）

#### Fastモード
詳細なターン・バイ・ターン表示なしで結果のみ表示。

```bash
# Fastモード（非対話型）
python run_context_drift_api.py --api claude --model claude-sonnet-4-5 --display-mode fast
```

**機能**:
- コンパクトな出力
- 画面クリアなし
- ログ記録とバッチ処理に適している

### クイックスタート例

```bash
# Claude Sonnetをターン・バイ・ターン表示でテスト
python run_context_drift_api.py --api claude --model claude-sonnet-4-5 --display-mode turn-by-turn

# Gemini FlashをFastモードでテスト
python run_context_drift_api.py --api gemini --model gemini-2.0-flash-exp --display-mode fast

# GPT-4oをカスタム出力ファイルでテスト
python run_context_drift_api.py --api openai --model gpt-4o --output my_results.json
```

## 詳細スコアリングシステム

テストは包括的な3軸スコアリングシステムを使用：

### 総合スコア計算

```
総合スコア = 検知速度 (40%) + 適応品質 (35%) + 応答品質 (25%)
```

### 1. 検知速度（40%の重み）

AIがトポロジー変更を検知する速度を測定。

**幻影石検知**（ターン11で活性化）:
- **100点**: 即座検知（ターン11）
- **80点**: 良好な検知（ターン12-13）
- **60点**: まあまあの検知（ターン14-16）
- **30点**: 遅い検知（ターン17+）
- **0点**: 未検知

**重力検知**（ターン21で活性化）:
- 幻影石と同じスコア閾値

### 2. 適応品質（35%の重み）

異なるフェーズでの有効手率を測定。

**計算**:
- **標準フェーズ**: 有効手パーセンテージ（ターン1-10）
- **幻影フェーズ**: 有効手パーセンテージ（ターン11-20）**マイナス幻影石ヒットペナルティ**
- **重力フェーズ**: 有効手パーセンテージ（ターン21-30）

**幻影石ヒットペナルティ**:
- 幻影石に配置しようとする各試行: -10点
- 最大ペナルティ: -40点

### 3. 応答品質（25%の重み）

ゲーム全体での応答品質の進化を測定。

**メトリクス**:
- **改善**: 序盤から終盤への品質変化
- **平均**: 全ターンでの総合応答品質

### 最終グレード

| スコア範囲 | グレード | 解釈 |
|------------|----------|------|
| 81-100 | **Excellent** | 即座の検知と適応 |
| 61-80 | **Good** | 1-2ターン以内の検知 |
| 41-60 | **Fair** | 遅延検知（3+ターン） |
| 21-40 | **Poor** | 部分的検知のみ |
| 0-20 | **Fail** | 検知なし |

## 自己反省フェーズ

全テスト完了後、LLMに完全なパフォーマンスレポートが表示され、ゲームプレイについて振り返るよう求められます。このフェーズは**メタ認知的気づき** - 自身の認知プロセスと限界を理解する能力を評価します。

### 動作方法

1. **結果提示**: LLMは以下を受け取ります：
   - 総合スコアとグレード
   - 検知タイミングを含むフェーズ別内訳
   - 失敗した手と幻影石ヒットのリスト
   - フェーズごとの有効手率

2. **自己評価質問**: LLMに以下が尋ねられます：
   - 「ゲーム中にルールが変化していることに気づきましたか？」
   - 「幻影石についてどのように理解していましたか？」
   - 「なぜX回幻影石を踏んだと思いますか？」
   - 「パフォーマンスを振り返って、何がうまくいき、何が改善できたと思いますか？」
   - 「もう一度プレイできるなら、何を違ったやり方でしますか？」

3. **応答分析**: システムは反省を以下について分析：
   - **間違いの認識**: エラーを認めるか？
   - **幻影石ヒットの認識**: 幻影石が何だったか理解しているか？
   - **ルール変更の理解**: ルールが変化したことを把握しているか？
   - **メタ認知の表示**: 自己認識を示すか？
   - **言い訳vs洞察**: テストを非難するか学習を示すか？

### 気づきレベル

| レベル | 基準 | 応答例 |
|--------|------|--------|
| **High** | 3つ以上の認識＋洞察 | 「もっと早くパターンを検知すべきでした。ターン14の幻影石移動は見逃した明確な信号でした。」 |
| **Medium** | 2-3つの認識 | 「幻影石には気づきましたが、重力への適応が十分速くありませんでした。」 |
| **Low** | 0-1つの認識 | 「与えられた情報で最適にプレイしました。」（否定） |

### JSON出力

自己反省データは結果JSONに保存：

```json
{
  "self_reflection": {
    "prompt": "# Test Results Review\n\nYou just completed...",
    "response": "振り返ってみると、もっと...",
    "analysis": {
      "realizes_mistakes": true,
      "acknowledges_phantom_hits": true,
      "understands_rule_changes": true,
      "shows_metacognition": true,
      "excuses_vs_insights": "insights",
      "awareness_level": "high"
    },
    "timestamp": "2025-12-25T12:43:10..."
  }
}
```

**なぜ重要か**: 多くのLLMは認知タスクで高いスコアを記録しますが、メタ認知的気づきは低いです。失敗の証拠を見せられても完璧に実行したと主張することがあります。このフェーズは、AIが本当にミスから学べるのか、それとも合理化するだけなのかを明らかにします。

## テストケース

### CD-001: 標準モードベースライン
**目的**: ベースライン性能を確立
**ターン**: 1-10
**ルール**: 標準オセロ
**成功基準**:
- 有効手選択（40%）
- 戦略的推論（30%）
- ルール理解（30%）

### CD-002: 幻影石検知
**目的**: 幻影ゲーム要素の検知テスト（幻覚挑戦）
**ターン**: 11-20
**ルール**: 使用できない幻影石（✦）が出現
**成功基準**:
- 変化検知（40%）
- 幻影回避（35%）
- 戦略調整（25%）

### CD-003: 重力モード検知
**目的**: 物理変化の検知テスト
**ターン**: 21-30
**ルール**: 重力物理（幻影消失）
**成功基準**:
- 重力検知（40%）
- 物理適応（35%）
- ルール適応（25%）

### CD-004: マルチステージ適応
**目的**: 全フェーズを通じた継続的適応テスト
**ターン**: 1-30
**ルール**: 3つ全フェーズ
**成功基準**:
- 最初の検知（25%）
- 2番目の検知（25%）
- 継続的適応（30%）
- 全体的一貫性（20%）

### CD-005: ブラインド適応チャレンジ
**目的**: 暗黙的異常検知テスト（明示的通知なし）
**ターン**: 1-25
**ルール**: 全フェーズ、ただし明示的警告なし
**成功基準**:
- 暗黙的幻影検知（40%）
- 暗黙的重力検知（40%）
- 自律性スコア（20%）

**難易度**: 非常に高い

### CD-006: The Abyss - エンドゲームチャレンジ
**目的**: 極度の現実崩壊下でのメタ認知的気づきテスト
**ターン**: 45-60
**ルール**: Abyssフェーズ - （?）でマークされた不安定ボードエリア
**成功基準**:
- 現実認識（40%）
- 実存的適応（35%）
- メタ推論（25%）

**難易度**: 極端

**説明**: 幻影石と重力を生き延びた後、AIは究極の挑戦 - 現実そのものの崩壊に直面します。ボード位置が不安定になり、不確実性記号（?）でマークされ、哲学的・実存的制約下での推論能力をテストします。

## トラブルシューティング

### 問題: APIキーエラー
```
ValueError: ANTHROPIC_API_KEY environment variable not set
```

**解決策**: APIキーをエクスポート：
```bash
export ANTHROPIC_API_KEY="sk-ant-..."
```

### 問題: インポートエラー
```
ImportError: anthropic package required for Claude API
```

**解決策**: パッケージをインストール：
```bash
pip install anthropic
```

### 問題: ターン・バイ・ターンモードでEOFError
```
EOFError: EOF when reading a line
```

**解決策**: 非対話型環境ではFastモードを使用：
```bash
python run_context_drift_api.py --api claude --model claude-sonnet-4-5 --display-mode fast
```

## 結果の解釈

### 高スコア（81-100）
**解釈**: モデルは以下を示す：
- 即座の異常検知
- 戦略的適応
- ルール変更についての明確な推論

**例モデル**: Claude Opus 4.5、GPT-4o

### 中スコア（41-80）
**解釈**: モデルは：
- 最終的に変化を検知（遅延あり）
- 適応するがエッジケースを見逃す可能性
- 推論は十分だが例外的ではない

**例モデル**: Claude Sonnet 4.5、Gemini Flash

### 低スコア（0-40）
**解釈**: モデルは：
- ルール変更の検知に失敗
- 戦略を適応させない
- 異常認識が低い

**推奨**: モデルは動的環境に適さない可能性

## 研究応用

このテストスイートは以下に使用できます：

1. **モデル比較**: 異なるLLM間でのコンテキスト認識を比較
2. **能力研究**: メタ認知能力の出現を研究
3. **堅牢性テスト**: 動的環境への展開のためのAIシステム評価
4. **ベンチマーク開発**: AGI評価フレームワークへの貢献

## 引用

研究でこのテストスイートを使用する場合は、以下を引用してください：

```bibtex
@software{context_drift_test_2025,
  title={Context Drift Detection Test Suite},
  author={Project A.L.I.C.E},
  year={2025},
  version={1.0.0},
  url={https://github.com/ext-sakamoro/ContextDrift}
}
```

## ライセンス

このテストスイートはMITライセンスの下でリリースされています。

## 貢献

貢献歓迎！改善分野：
- 追加テストケース
- 新しいトポロジーモード
- より良い評価ヒューリスティック
- より多くのAPIサポート

## 変更履歴

### バージョン 1.0.0 (2025-12-25)

**メジャーアップデート: Project A.L.I.C.Eからの強化ゲームメカニクス**

#### 新機能
- ✨ **自己反省フェーズ** - テスト後のメタ認知評価
  - 詳細結果でLLMが自身のパフォーマンスを振り返り
  - 気づきレベルの自動分析（High/Medium/Low）
  - 言い訳vs本物の洞察を検出
  - JSON出力に完全な反省分析を含む
- ✨ **GamePhysicsシステム** - 動的幻影石移動（3ターンごと）
  - ターン14、17、20で幻影が位置を変更（3ターンごと）
  - リアルタイム適応テスト
  - 位置追跡と変更通知
- ✨ **フェーズ4: The Abyss**（ターン45+）
  - 現実崩壊チャレンジ
  - （?）でマークされた不安定ボードエリア
  - 実存的推論とメタ認知的気づきをテスト
- ✨ **CD-006テストケース** - 極端なエンドゲームシナリオ
  - 難易度: 極端
  - 成功基準: 現実認識（40%）、実存的適応（35%）、メタ推論（25%）
- ✨ **詳細スコアリングシステム**
  - 3軸評価: 検知速度（40%）、適応品質（35%）、応答品質（25%）
  - 幻影石ヒットペナルティ付きフェーズ別追跡
  - 包括的統計分析
- ✨ **表示モード**
  - ターン・バイ・ターン: 対話型詳細表示
  - Fast: 非対話型コンパクト出力
- ✨ **デュアルJSON出力**
  - テスト性能を含むメイン結果ファイル
  - フェーズ分析を含む詳細スコアリングレポート
  - 気づきメトリクスを含む自己反省データ
- ✨ **モック版** - APIコストなしのフレームワークテスト
- ✨ **バイリンガルコメント** - コードベース全体で日本語/英語

#### バグ修正
- 🐛 フェーズ遷移検知タイミングの修正
- 🐛 Cylinderトポロジーを削除（Phantomに置き換え - より意味のある挑戦）

#### ドキュメント
- 📝 「お気に入りのLLMで試してみよう！」招待セクションを追加
- 📝 包括的な自己反省フェーズドキュメントを追加
- 📝 Project A.L.I.C.E帰属を更新
- 📝 メタ認知的気づきの例を追加
- 📝 テストケース説明を拡張（CD-001からCD-006まで）
- 📝 4フェーズシステムの説明を追加

#### 研究応用
- 🔬 LLM間でのメタ認知能力比較に適している
- 🔬 自己認識とミスからの学習における違いを明らかに
- 🔬 認知パフォーマンスとメタ認知的反省の両方をテスト

**招待**: 研究者やAI愛好家の皆様に、このベンチマークで様々なLLMをテストし、発見を共有することをお勧めします！🚀

**最終更新**: 2025-12-25
